"""Sample-level hard filtering based on Picard statistics"""

import os
import logging
import csv
import hail as hl
import pandas as pd

from joint_calling.utils import gs_cache_file, file_exists


logger = logging.getLogger('sample_qc_hard_filtering')


def compute_hard_filters(
    mt: hl.MatrixTable,
    qc_ht: hl.MatrixTable,
    sex_ht: hl.Table,
    hail_sample_qc_ht: hl.Table,
    out_ht_path: str,
    cov_threshold: int,
    overwrite: bool = False,
) -> hl.Table:
    """
    Uses the sex imputation results, results of the sample_qc() run on
    bi-allelic variants, and Picard stats files specificed in `sample_df`,
    to apply filters to samples in `mt` and create a table with
    samples that fail at least one sampe.

    :param mt: input matrix table
    :param qc_ht: QC metadata generated by combine_gvcfs. Expected fields:
        contamination, alignment_summary_metrics, duplicate_metrics,
        insert_size_metrics, wgs_metrics (any of those are optional).
        Values must point to corresponding Picard stats files (see
        `_parse_metrics` for details)
    :param sex_ht: required fields: "sex_karyotype", "chr20_mean_dp"
    :param hail_sample_qc_ht: required fields:
        "bi_allelic_sample_qc { n_snp, n_singleton, r_het_hom_var }"
    :param out_ht_path: location to write the hard filtered samples Table
    :param cov_threshold: minimal chr20 coverage
    :param overwrite: overwrite checkpoints if they exist
    :return: table with samples failed the filters, and the following structure:
        's': str
        'hard_filters': set<str>  # a non-empty subset of { ambiguous_sex,
            sex_aneuploidy,  low_coverage, bad_biallelic_metrics, contamination,
            chimera, coverage, insert_size }
    """
    logger.info('Generating hard filters')
    if not overwrite and file_exists(out_ht_path):
        return hl.read_table(out_ht_path)

    ht = mt.cols()
    ht = ht.annotate(hard_filters=hl.empty_set(hl.tstr))

    # Helper function to add filters into the `hard_filters` set
    def add_filter(ht, expr, name):
        return ht.annotate(
            hard_filters=hl.if_else(
                expr & hl.is_defined(expr), ht.hard_filters.add(name), ht.hard_filters
            )
        )

    # Remove samples with ambiguous sex assignments
    ht = add_filter(ht, sex_ht[ht.key].sex_karyotype == 'ambiguous', 'ambiguous_sex')
    ht = add_filter(
        ht,
        ~hl.set({'ambiguous', 'XX', 'XY'}).contains(sex_ht[ht.key].sex_karyotype),
        'sex_aneuploidy',
    )

    # Remove low-coverage samples
    # chrom 20 coverage is computed to infer sex and used here
    ht = add_filter(ht, sex_ht[ht.key].chr20_mean_dp < cov_threshold, 'low_coverage')

    # Remove extreme raw bi-allelic sample QC outliers
    ht = add_filter(
        ht,
        (
            (hail_sample_qc_ht[ht.key].bi_allelic_sample_qc.n_snp > 8.0e6)
            | (hail_sample_qc_ht[ht.key].bi_allelic_sample_qc.n_snp < 2.4e6)
            | (hail_sample_qc_ht[ht.key].bi_allelic_sample_qc.n_singleton > 3e5)
            | (hail_sample_qc_ht[ht.key].bi_allelic_sample_qc.r_het_hom_var > 3.3)
        ),
        'bad_biallelic_metrics',
    )

    # Remove samples that fail picard metric thresholds, percents are not divided
    # by 100, e.g. 5% == 5.00, 5% != 0.05
    ht = add_filter(ht, qc_ht[ht.key].freemix > 5.00, 'contamination')
    ht = add_filter(ht, qc_ht[ht.key].pct_chimeras > 5.00, 'chimera')
    ht = add_filter(ht, qc_ht[ht.key].mean_coverage < 15.0, 'coverage')
    ht = add_filter(ht, qc_ht[ht.key].median_insert_size < 250.9, 'insert_size')
    ht = ht.filter(hl.len(ht.hard_filters) > 0)
    ht.write(out_ht_path, overwrite=True)
    return ht


def _parse_picard_metrics(metadata_ht: hl.Table, local_tmp_dir: str) -> hl.Table:
    """
    Reads Picard stats files from `metadata_ht`, and converts relevant
    stats into a Hail table.

    :param metadata_ht: metadata generated by combine_gvcfs.
        Expected one of 2 formats:
        1. Fields that point to Picard stats files QC file locations
        (generated by the WARP pipeline):
        contamination, alignment_summary_metrics, duplicate_metrics,
        insert_size_metrics, wgs_metrics
        * `contamination` expected to point to a file like:
          `call-UnmappedBamToAlignedBam/UnmappedBamToAlignedBam/*/
           call-CheckContamination/*.selfSM`, and extract the metric `FREEMIX`
        * `alignment_summary_metrics` ->
          `call-AggregatedBamQC/AggregatedBamQC/*/call-CollectAggregationMetrics/
           *.alignment_summary_metrics`, extract `PCT_CHIMERAS`
        * `duplicate_metrics` ->
          `call-UnmappedBamToAlignedBam/UnmappedBamToAlignedBam/*/
           call-MarkDuplicates/*.duplicate_metrics`, extract `PERCENT_DUPLICATION`
        * `median_insert_size` ->
          `call-AggregatedBamQC/AggregatedBamQC/*/call-CollectAggregationMetrics/
           *.insert_size_metrics`, extact `MEDIAN_INSERT_SIZE`
        * `wgs_metrics` ->
          `call-CollectWgsMetrics/*.wgs_metrics`, extract `MEDIAN_COVERAGE`

        2. Fields with metric values (from the KCG pipeline)
        sample.sample_name,
        raw_data.FREEMIX,
        raw_data.PlinkSex,
        raw_data.PCT_CHIMERAS,
        raw_data.PERCENT_DUPLICATION,
        raw_data.MEDIAN_INSERT_SIZE,
        raw_data.MEDIAN_COVERAGE
        FG1529,0.0098939700,F(-1),0.023731,0.151555,412.0,31.0
    :return: a table with the folliwing structure:
        "s":                  hl.tstr,
        "freemix":            hl.tfloat32,
        "pct_chimeras":       hl.tfloat32,
        "duplication":        hl.tfloat32,
        "median_insert_size": hl.tint32,
        "mean_coverage":      hl.tint32
    """

    data = []
    for row in metadata_ht.collect():
        if 'sample' in row:
            d = dict(s=row.sample)

            contam = row.get('contamination')
            d['freemix'] = _parse_picard_metric(contam, 'FREEMIX', local_tmp_dir)

            aln_sum_metrics = row.get('alignment_summary_metrics')
            d['pct_chimeras'] = _parse_picard_metric(
                aln_sum_metrics, 'PCT_CHIMERAS', local_tmp_dir
            )

            dup_metrics = row.get('duplicate_metrics')
            d['duplication'] = _parse_picard_metric(
                dup_metrics, 'PERCENT_DUPLICATION', local_tmp_dir
            )

            is_metrics = row.get('insert_size_metrics')
            d['median_insert_size'] = _parse_picard_metric(
                is_metrics, 'MEDIAN_INSERT_SIZE', local_tmp_dir
            )

            wgs_metrics = row.get('wgs_metrics')
            d['mean_coverage'] = _parse_picard_metric(
                wgs_metrics, 'MEDIAN_COVERAGE', local_tmp_dir
            )

            data.append(d)

        elif 'sample.sample_name' in row:
            d = dict(s=row['sample.sample_name'])
            d['freemix'] = float(row['raw_data.FREEMIX'])
            d['pct_chimeras'] = float(row['raw_data.PCT_CHIMERAS'])
            d['duplication'] = float(row['raw_data.PERCENT_DUPLICATION'])
            d['median_insert_size'] = round(float(row['raw_data.MEDIAN_INSERT_SIZE']))
            d['mean_coverage'] = round(float(row['raw_data.MEDIAN_COVERAGE']))
            data.append(d)

    csv_path = os.path.join(local_tmp_dir, 'sample_qc_metrics.csv')
    with open(csv_path, 'w', newline='') as out:
        writer = csv.DictWriter(out, fieldnames=data[0].keys())
        writer.writeheader()
        writer.writerows(data)

    ht = hl.import_table(
        csv_path,
        delimiter=',',
        key='s',
        types={
            's': hl.tstr,
            'freemix': hl.tfloat32,
            'pct_chimeras': hl.tfloat32,
            'duplication': hl.tfloat32,
            'median_insert_size': hl.tint32,
            'mean_coverage': hl.tint32,
        },
    )
    return ht


def _parse_picard_metric(fpath, metric_name, local_tmp_dir):
    val = 'NA'
    if not fpath or pd.isnull(fpath):
        return val
    with open(gs_cache_file(fpath, local_tmp_dir)) as fh:
        idx = None
        for line in fh:
            if f'\t{metric_name}\t' in line:
                idx = line.split('\t').index(metric_name)
                continue
            if idx is not None:
                val = line.split('\t')[idx]
                try:
                    val = int(val)
                except ValueError:
                    try:
                        val = float(val)
                    except ValueError:
                        pass
                break
    return val
